{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_caPfpsuqaa",
        "outputId": "7527670b-1bc7-4ead-891c-3814aae10922"
      },
      "source": [
        "!pip install jsonlines\n",
        "!pip install tensorflow\n",
        "\n",
        "import jsonlines\n",
        "import json\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "import re\n",
        "import numpy as np\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn import metrics\n",
        "import collections\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM,Flatten\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting jsonlines\n",
            "  Downloading https://files.pythonhosted.org/packages/d4/58/06f430ff7607a2929f80f07bfd820acbc508a4e977542fefcc522cde9dff/jsonlines-2.0.0-py3-none-any.whl\n",
            "Installing collected packages: jsonlines\n",
            "Successfully installed jsonlines-2.0.0\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.4.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.32.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.4.1)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow) (56.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (2.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (0.4.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.30.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (3.3.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.12.5)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow) (4.0.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.4.1)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gERN6P6vNhF",
        "outputId": "c79e2754-3384-45f5-9126-0319d55a68f8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsN_9Np5vT1r"
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/My Drive\") "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zu4AxCNnuqag"
      },
      "source": [
        "raw_train_data = []\n",
        "with open(\"project-data/train.data.jsonl\", \"r+\") as f:\n",
        "    for each_item in jsonlines.Reader(f):\n",
        "        raw_train_data.append(each_item)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpVZD2h19lfL"
      },
      "source": [
        "raw_dev_data = []\n",
        "with open(\"project-data/dev.data.jsonl\", \"r+\") as f:\n",
        "    for each_item in jsonlines.Reader(f):\n",
        "        raw_dev_data.append(each_item)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWeVVLzcQKlR"
      },
      "source": [
        "raw_test_data = []\n",
        "with open(\"project-data/test.data.jsonl\", \"r+\") as f:\n",
        "    for each_item in jsonlines.Reader(f):\n",
        "        raw_test_data.append(each_item)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eU7ZyKM1uqah"
      },
      "source": [
        "with open(\"project-data/train.label.json\", \"r+\") as f:\n",
        "    raw_train_label = json.load(f)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqXjM1bcuqai"
      },
      "source": [
        "with open(\"project-data/dev.label.json\", \"r+\") as f:\n",
        "    raw_dev_label = json.load(f)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BLE2MBkQKlR"
      },
      "source": [
        "with open(\"project-data/test-output.json\", \"r+\") as f:\n",
        "    sample_test_label = json.load(f)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOWvG87zuqak"
      },
      "source": [
        "def text_filter(t):\n",
        "    t = t.lower()\n",
        "    t = re.sub(r\"(@)\\S+\", \"\", t)\n",
        "    t = re.sub(r'_',' ', t)\n",
        "    t = re.sub(r'\\d','', t)\n",
        "    t = re.sub(r'\\-', ' ', t)\n",
        "    t = re.sub(r'\\.', '', t)\n",
        "    t = re.sub(r'\\\\', ' ', t)\n",
        "    t = re.sub(r'\\\\x\\.+', '', t)\n",
        "    t = re.sub(r'^_.', '', t)\n",
        "    t = re.sub(r'^ ', '', t)\n",
        "    t = re.sub(r' $', '', t)\n",
        "    t = re.sub(r'\\#\\.', '', t)\n",
        "    t = re.sub(r'&', '', t)\n",
        "    t = re.sub(r'\\n', '', t)\n",
        "    t = re.sub(r',', '', t)\n",
        "    t = re.sub(r\"\\n\", \"\", t)\n",
        "    t = re.sub(r\"(https?\\://|www)\\S+\", \"\", t)\n",
        "    t = re.sub(r'\\?', '', t) \n",
        "    t = re.sub(r'it\\'s','it is', t)\n",
        "    t = re.sub(r'i\\'m','i am', t)\n",
        "    t = re.sub(r'she\\'s','she is', t)\n",
        "    t = re.sub(r'he\\'s','he is', t)\n",
        "    t = re.sub(r'that\\'s','that is', t)\n",
        "    t = re.sub(r'they\\'re','they are', t)\n",
        "    t = re.sub(r'ain\\'t','am not', t)\n",
        "    t = re.sub(r'can\\'t','can not', t)\n",
        "    t = re.sub(r'could\\'t','could not', t)\n",
        "    t = re.sub(r'won\\'t','will not', t)\n",
        "    t = re.sub(r'wouldn\\'t','would not', t)\n",
        "    t = re.sub(r'should\\'t','should not', t)\n",
        "    t = re.sub(r'there\\'s','there is', t)\n",
        "    t = re.sub(r'what\\'s','what is', t)\n",
        "    t = t.strip()\n",
        "    return t"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEIgi3FDuqak"
      },
      "source": [
        "t_tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
        "def raw_data_preprocessing(raw_data):\n",
        "    data_processed = []\n",
        "    for each_item in raw_data:\n",
        "        processed_text_temp = []   \n",
        "        processed_word = text_filter(each_item)\n",
        "        processed_word = t_tokenizer.tokenize(processed_word)\n",
        "        for each_word in processed_word:\n",
        "            if (len(each_word) != 0) and (re.search(r\"[a-z]+\", each_word) is not None) and (each_word not in set(stopwords.words('english'))): \n",
        "                processed_text_temp.append(each_word)\n",
        "        data_processed.append(processed_text_temp)\n",
        "    #print(data_processed[0])    \n",
        "    return data_processed\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSWixnsvuqak"
      },
      "source": [
        "raw_train_data_conc = []\n",
        "for i in raw_train_data:\n",
        "    each_twitter_text = \" \"\n",
        "    for j in i:\n",
        "        each_twitter_text = each_twitter_text + j['text']\n",
        "    raw_train_data_conc.append(each_twitter_text)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "HG-713sEQKlT",
        "outputId": "a36fddc3-c608-4cb1-b368-5a03e0148929"
      },
      "source": [
        "each_twitter_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Prayers for #Ferguson.@BET @MTV #INMTW http://t.co/s21rhqCKQN@laddygagha @BET hummm“@BET: Prayers for #Ferguson.” prayers for #MikeBrown family@BET http://t.co/nSs0kkAN8t@BET Our thoughts and prayers. #Ferguson“@BET: Prayers for #Ferguson.” http://t.co/52pILjsFPv@BET #Anonymous has released the names@BET AMEN!!!\\n#Praying@bet BUY 20K followers for ONLY $50 http://t.co/bmYZrv882s@BET Pray For Us #Ferguson  #JusticeForMikeBrown“@BET: Prayers for #Ferguson.” And his family and friends! Especially the kid with him at the shooting! #prayers #godhelpourcommunities“@BET: Prayers for #Ferguson.” Unhelpful tweets.“@BET: Prayers for #Ferguson.”'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVGmUvFQuqal"
      },
      "source": [
        "raw_dev_data_conc = []\n",
        "for i in raw_dev_data:\n",
        "    each_twitter_text = \" \"\n",
        "    for j in i:\n",
        "        each_twitter_text = each_twitter_text + j['text']\n",
        "    raw_dev_data_conc.append(each_twitter_text)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tH9lPPpQKlU"
      },
      "source": [
        "raw_test_data_conc = []\n",
        "for i in raw_test_data:\n",
        "    each_twitter_text = \" \"\n",
        "    for j in i:\n",
        "        each_twitter_text = each_twitter_text + j['text']\n",
        "    raw_test_data_conc.append(each_twitter_text)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VYIHeUZuqal"
      },
      "source": [
        "processed_train_data = raw_data_preprocessing(raw_train_data_conc)\n",
        "processed_dev_data = raw_data_preprocessing(raw_dev_data_conc)\n",
        "processed_test_data = raw_data_preprocessing(raw_test_data_conc)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONCQYFYLuqal"
      },
      "source": [
        "tokenizer = Tokenizer(lower=True,split=',')\n",
        "tokenizer.fit_on_texts(processed_train_data)\n",
        "encoded_train_data = tokenizer.texts_to_sequences(processed_train_data)\n",
        "encoded_dev_data = tokenizer.texts_to_sequences(processed_dev_data)\n",
        "encoded_test_data = tokenizer.texts_to_sequences(processed_test_data)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzZntXnFuqal"
      },
      "source": [
        "# Padding\n",
        "pad_train_data = pad_sequences(encoded_train_data)\n",
        "pad_dev_data = pad_sequences(encoded_dev_data, maxlen = len(pad_train_data[0]))\n",
        "pad_test_data = pad_sequences(encoded_test_data, maxlen = len(pad_train_data[0]))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CG31yVpJQKlV"
      },
      "source": [
        "dev_labels_encoded = []\n",
        "for i in raw_dev_label.values():\n",
        "    if i == 'non-rumour':     \n",
        "        dev_labels_encoded.append(0)     \n",
        "    else:     \n",
        "        dev_labels_encoded.append(1)      "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sl54uLYBuqam"
      },
      "source": [
        "# Oversampling"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3s66mo4uqai",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bf12ba4-0316-4b00-9d78-490bd3252db8"
      },
      "source": [
        "train_labels_encoded = []\n",
        "negative_label_indexes = []\n",
        "positive_label_indexes = []\n",
        "j=-1\n",
        "for i in raw_train_label.values():\n",
        "    j+=1\n",
        "    if i == 'non-rumour':\n",
        "        train_labels_encoded.append(0)\n",
        "        negative_label_indexes.append(j)\n",
        "    else:\n",
        "        train_labels_encoded.append(1)\n",
        "        positive_label_indexes.append(j)\n",
        "print('Labeled Rumour: Non Rumour', sum(train_labels_encoded)/(len(train_labels_encoded)-sum(train_labels_encoded)))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Labeled Rumour: Non Rumour 0.5176586003924133\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywcUNrBduqam",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12cf7a51-f366-408b-e222-560d622a4181"
      },
      "source": [
        "oversampled_negative_sample_indexes = np.random.choice(negative_label_indexes,sum(train_labels_encoded),replace=False)\n",
        "oversampled_training_data=[]\n",
        "oversampled_training_labels=[]\n",
        "\n",
        "for each_index in oversampled_negative_sample_indexes:\n",
        "      oversampled_training_labels.append(train_labels_encoded[each_index])\n",
        "      oversampled_training_data.append(pad_train_data[each_index])\n",
        "\n",
        "for each_index in positive_label_indexes:\n",
        "      oversampled_training_labels.append(train_labels_encoded[each_index])\n",
        "      oversampled_training_data.append(pad_train_data[each_index])\n",
        "\n",
        "sum(oversampled_training_labels)/len(oversampled_training_labels)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZ_58DOTuqam"
      },
      "source": [
        "np_train_data = np.append(pad_train_data, np.array(oversampled_training_data), axis=0)\n",
        "np_train_labels = np.append(train_labels_encoded, np.array(oversampled_training_labels), axis=0)\n",
        "dev_labels_encoded = np.array(dev_labels_encoded)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avk9meeMuqao"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.Input(shape=(None,), dtype=\"int32\"),\n",
        "    tf.keras.layers.Embedding(2000000, 128),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Tx3dTLvuqao"
      },
      "source": [
        "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcX9amJouqao",
        "outputId": "950105aa-c190-4424-f676-48a5500ab59c"
      },
      "source": [
        "len(dev_labels_encoded)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "580"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z90AhrgxQKlY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9a4acfc-5cd6-49b7-9fec-5eb83e5199fd"
      },
      "source": [
        "len(pad_dev_data)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "580"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjAxUUWvuqap"
      },
      "source": [
        "model.save('./lstm_Concatenated.h5')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPoYImqYuqap",
        "outputId": "e0fa6051-38e6-46b5-ac76-f348df77cf83"
      },
      "source": [
        "model.fit(np_train_data, np_train_labels, batch_size=128, epochs=20, validation_data=(pad_dev_data, dev_labels_encoded))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "61/61 [==============================] - 543s 9s/step - loss: 0.6345 - accuracy: 0.6343 - val_loss: 0.4527 - val_accuracy: 0.8207\n",
            "Epoch 2/20\n",
            "61/61 [==============================] - 537s 9s/step - loss: 0.2196 - accuracy: 0.9251 - val_loss: 0.7309 - val_accuracy: 0.8155\n",
            "Epoch 3/20\n",
            "61/61 [==============================] - 550s 9s/step - loss: 0.0472 - accuracy: 0.9889 - val_loss: 1.1334 - val_accuracy: 0.7914\n",
            "Epoch 4/20\n",
            "61/61 [==============================] - 558s 9s/step - loss: 0.0196 - accuracy: 0.9963 - val_loss: 1.1918 - val_accuracy: 0.8000\n",
            "Epoch 5/20\n",
            "61/61 [==============================] - 559s 9s/step - loss: 0.0148 - accuracy: 0.9974 - val_loss: 1.2821 - val_accuracy: 0.8017\n",
            "Epoch 6/20\n",
            "61/61 [==============================] - 562s 9s/step - loss: 0.0050 - accuracy: 0.9989 - val_loss: 1.2805 - val_accuracy: 0.7983\n",
            "Epoch 7/20\n",
            "61/61 [==============================] - 561s 9s/step - loss: 0.0037 - accuracy: 0.9992 - val_loss: 1.5269 - val_accuracy: 0.8017\n",
            "Epoch 8/20\n",
            "61/61 [==============================] - 563s 9s/step - loss: 0.0037 - accuracy: 0.9988 - val_loss: 1.1678 - val_accuracy: 0.7897\n",
            "Epoch 9/20\n",
            "61/61 [==============================] - 561s 9s/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 1.4525 - val_accuracy: 0.8017\n",
            "Epoch 10/20\n",
            "61/61 [==============================] - 566s 9s/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 1.7172 - val_accuracy: 0.8086\n",
            "Epoch 11/20\n",
            "61/61 [==============================] - 570s 9s/step - loss: 9.2591e-04 - accuracy: 0.9996 - val_loss: 1.6938 - val_accuracy: 0.8034\n",
            "Epoch 12/20\n",
            "61/61 [==============================] - 559s 9s/step - loss: 8.3110e-05 - accuracy: 1.0000 - val_loss: 1.8354 - val_accuracy: 0.8017\n",
            "Epoch 13/20\n",
            "61/61 [==============================] - 577s 9s/step - loss: 6.0271e-05 - accuracy: 1.0000 - val_loss: 1.8793 - val_accuracy: 0.8034\n",
            "Epoch 14/20\n",
            "61/61 [==============================] - 576s 9s/step - loss: 4.8015e-05 - accuracy: 1.0000 - val_loss: 1.9210 - val_accuracy: 0.8069\n",
            "Epoch 15/20\n",
            "61/61 [==============================] - 571s 9s/step - loss: 4.0069e-05 - accuracy: 1.0000 - val_loss: 1.9650 - val_accuracy: 0.8086\n",
            "Epoch 16/20\n",
            "61/61 [==============================] - 568s 9s/step - loss: 3.7642e-05 - accuracy: 1.0000 - val_loss: 2.0057 - val_accuracy: 0.8086\n",
            "Epoch 17/20\n",
            "61/61 [==============================] - 569s 9s/step - loss: 3.1863e-05 - accuracy: 1.0000 - val_loss: 2.0418 - val_accuracy: 0.8086\n",
            "Epoch 18/20\n",
            "61/61 [==============================] - 573s 9s/step - loss: 2.5232e-05 - accuracy: 1.0000 - val_loss: 2.0789 - val_accuracy: 0.8103\n",
            "Epoch 19/20\n",
            "61/61 [==============================] - 581s 10s/step - loss: 2.1798e-05 - accuracy: 1.0000 - val_loss: 2.1088 - val_accuracy: 0.8103\n",
            "Epoch 20/20\n",
            "61/61 [==============================] - 583s 10s/step - loss: 2.9050e-05 - accuracy: 1.0000 - val_loss: 2.1380 - val_accuracy: 0.8103\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9d39d06d50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIl5a6P_uqap"
      },
      "source": [
        "dev_pre_freq = model.predict(pad_dev_data)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZudKe-2QKlY"
      },
      "source": [
        "test_pre_freq = model.predict(pad_test_data)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_m7AmxBCuqap"
      },
      "source": [
        "dev_pred = []\n",
        "for i in dev_pre_freq:\n",
        "    if i > 0.5:\n",
        "        dev_pred.append(1)\n",
        "    else:\n",
        "        dev_pred.append(0)\n",
        "dev_pred = np.array(dev_pred)        "
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "287pWmzSQKlZ"
      },
      "source": [
        "test_pred = []\n",
        "for i in test_pre_freq:\n",
        "    if i > 0.5:\n",
        "        test_pred.append(1)\n",
        "    else:\n",
        "        test_pred.append(0)\n",
        "test_pred = np.array(test_pred)    "
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Hfw88lfuqap",
        "outputId": "36fe4a92-693b-4830-e69e-dec23dc24a16"
      },
      "source": [
        "print(metrics.classification_report(dev_labels_encoded, dev_pred))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.87       393\n",
            "           1       0.75      0.63      0.68       187\n",
            "\n",
            "    accuracy                           0.81       580\n",
            "   macro avg       0.79      0.76      0.77       580\n",
            "weighted avg       0.81      0.81      0.81       580\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeWZA6vxuqaq"
      },
      "source": [
        "test_pred_json = dict()\n",
        "for i in range(len(sample_test_label.keys())):\n",
        "    if test_pred[i] == 1:\n",
        "        label_value = 'rumour'\n",
        "    else:\n",
        "        label_value = 'non-rumour'\n",
        "    test_pred_json[list(sample_test_label.keys())[i]] = label_value"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ku1VFptjQKlZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46410639-25ba-4b6e-a376-aba5078ea344"
      },
      "source": [
        "with open(\"test-output-2.json\",\"w\") as f:  \n",
        "    json.dump(dict(test_pred_json),f)\n",
        "    print(\"test-output-2.json Saved Successfully!\")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test-output-2.json Saved Successfully!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}